# -*- coding: utf-8 -*-
"""Rupesh_Capstone_BaselineModels_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18EwWVFETED3DNmpIHiCEtdmmpfbYgpBt

## Install Required Libraries
"""
"""## Import Required Libraries"""

import pendulum
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from airflow.decorators import dag, task
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import warnings
import itertools
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "start_date": pendulum.datetime(2025, 3, 1, 0, 0),
    "end_date": pendulum.datetime(2025, 6, 16, 0, 0),  # Stop after 15 minutes
}
@dag(
    schedule="@daily",
    default_args=default_args,
    tags=["Rupesh"],
    catchup=False
)
def base_model():
        
    """## Load Data"""
    @task()
    def extract():
        # Download NIFTY50 data
        ticker = "^NSEI"  # NIFTY50 index
        df = yf.download(ticker, start="2010-01-01", end="2025-03-01")

        # Use only the closing price
        data = df[['Close']].dropna()
        plt.figure(figsize=(12, 6))
        plt.plot(data)
        plt.title("NIFTY50 Stock Prices")
        plt.show()

        data.shape

        # Save data to csv
        data.to_csv("~/ML_spark/dag_spark/Causal_Alpha/dataset/base_models/nifty50.csv")
        return "extracted successfully"
    
    @task()
    def train_ARIMA(status):
        # Use last 100 days data for Test & remaining for training
        data = pd.read_csv('~/ML_spark/dag_spark/Causal_Alpha/dataset/base_models/nifty50.csv')
        data_last_100 = data.iloc[-100:]
        data = data.iloc[:-100]
        # plot data & data_last_100
        plt.figure(figsize=(12, 6))
        plt.plot(data.index, data['Close'], label="NIFTY50 Stock Prices")
        plt.plot(data_last_100.index, data_last_100['Close'], label="Last 100 Days Prices")
        plt.title("NIFTY50 Stock Prices")
        plt.legend()
        plt.show()

        """## Baseline Models
        ### ARIMA Model
        """
        warnings.filterwarnings("ignore")

        """ Determine optimum differencing order (d) using ADF test"""

        # ADF Test Function
        def adf_test(series):
            result = adfuller(series)
            print("ADF Statistic:", result[0])
            print("p-value:", result[1])
            return result[1] > 0.05  # If p > 0.05, data is non-stationary

        # Check stationarity for different differencing levels
        diff_data = data['Close'].copy()
        d = 0
        while adf_test(diff_data):
            d += 1
            diff_data = diff_data.diff().dropna()

        print(f"Optimal Differencing Order (d): {d}")

        """Find p (AR order) & q (MA order) using PACF & ACF"""

        # Plot ACF & PACF
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # ACF Plot (for q)
        plot_acf(diff_data, ax=axes[0], lags=30)
        axes[0].set_title("Autocorrelation (ACF) for Moving Average (q)")

        # PACF Plot (for p)
        plot_pacf(diff_data, ax=axes[1], lags=30)
        axes[1].set_title("Partial Autocorrelation (PACF) for AutoRegressive (p)")

        plt.show()

        """Using Grid Search to find (p,d,q)"""

        

        # Possible values for p, d, q
        p_range = range(0, 5)
        d_range = [d]  # From ADF test
        q_range = range(0, 5)

        # Grid search
        best_aic = float("inf")
        best_order = None
        best_model = None

        for p, d, q in itertools.product(p_range, d_range, q_range):
            try:
                model = ARIMA(data['Close'], order=(p, d, q))
                result = model.fit()
                if result.aic < best_aic:
                    best_aic = result.aic
                    best_order = (p, d, q)
                    best_model = result
            except:
                continue

        print(f"Best ARIMA Order: {best_order} with AIC: {best_aic}")

        # Forecast the next 100 days using the best ARIMA model
        future_steps = 100
        forecast_result = best_model.get_forecast(steps=future_steps)
        forecast_values = forecast_result.predicted_mean
        forecast_ci = forecast_result.conf_int()

        # Generate future dates
        # future_dates = pd.date_range(start=data.index[-1], periods=future_steps + 1)[1:]
        future_dates = data_last_100.index

        # Convert forecast results to DataFrame
        forecast_df = pd.DataFrame({
            "Date": future_dates,
            "Predicted_Close": forecast_values.values,
            "Lower_Bound": forecast_ci.iloc[:, 0].values,
            "Upper_Bound": forecast_ci.iloc[:, 1].values
        })

        # Plot Historical vs Forecasted Prices
        plt.figure(figsize=(12, 6))
        # plt.plot(df.index[-100:], df['Close'].values[-100:], label="Past 100 Days Prices")
        plt.plot(data_last_100.index, data_last_100.values, label="Past 100 Days Prices")
        plt.plot(future_dates, forecast_values, label="ARIMA Predictions", linestyle="dashed", color='red')
        plt.fill_between(future_dates, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='gray', alpha=0.2)
        plt.title("Future Stock Price Predictions (Next 100 Days) - ARIMA")
        plt.legend()
        plt.show()

        # Print Predicted Prices
        print(forecast_df)

        forecast_df["Actual_Close"] = data_last_100.values
        forecast_df["Error"] = forecast_df["Predicted_Close"] - forecast_df["Actual_Close"]
        forecast_df["Error_Percentage"] = (forecast_df["Error"] / forecast_df["Actual_Close"]) * 100

        # Plot the Date vs Error Percentage
        plt.figure(figsize=(12, 6))
        plt.plot(forecast_df['Date'], forecast_df['Error_Percentage'], label="Past 100 Days Error Percentage")
        plt.title("Error Percentage vs Date - ARIMA")
        plt.xlabel("Date")
        plt.ylabel("Error Percentage")
        return forecast_values
        """### LSTM Model"""

        # Loading entire data again
        # data = df[['Close']].dropna()
        # data.shape
    @task()
    def train_LSTM(forecast_values):
        # Create sequences
        def create_sequences(data, seq_length):
            X, y = [], []
            for i in range(len(data) - seq_length):
                X.append(data[i:i+seq_length])
                y.append(data.iloc[i+seq_length])
            return np.array(X), np.array(y)

        seq_length = 60  # 60 days window
        X, y = create_sequences(data, seq_length)

        # Create a new DF with created sequence & save into csv
        data_seq = pd.DataFrame(X.reshape(X.shape[0], -1),  columns=[f"Day_{i+1}" for i in range(X.shape[1])]
                                , index=data.index[seq_length:], dtype=float)

        data_seq.tail()

        data_seq.shape

        X_final, y_final = X[:-100], y[:-100]
        X_last_100, y_last_100 = X[-100:], y[-100:]
        X_final.shape, y_final.shape, X_last_100.shape, y_last_100.shape

        # Split data into train, test
        train_size = int(0.8 * len(X_final))
        X_train, X_test = X_final[:train_size], X_final[train_size:]
        y_train, y_test = y_final[:train_size], y_final[train_size:]

        # Reshape X_train and X_test to 2D before scaling
        X_train_2D = X_train.reshape(X_train.shape[0], -1)  # Reshape to 2D for scaling
        X_test_2D = X_test.reshape(X_test.shape[0], -1)    # Reshape to 2D for scaling
        X_last_100_2D = X_last_100.reshape(X_last_100.shape[0], -1)

        # Data Scaling
        scaler = MinMaxScaler(feature_range=(0, 1))
        X_train_scaled = scaler.fit_transform(X_train_2D)
        X_test_scaled = scaler.transform(X_test_2D)
        X_last_100_scaled = scaler.transform(X_last_100_2D)

        y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))
        y_test_scaled = scaler.transform(y_test.reshape(-1, 1))
        y_last_100_scaled = scaler.transform(y_last_100.reshape(-1, 1))

        # Reshape back to original shape
        X_train_scaled = X_train_scaled.reshape(X_train.shape)  # Reshape back to (2849, 60, 1)
        X_test_scaled = X_test_scaled.reshape(X_test.shape)    # Reshape back to (713, 60, 1)
        X_last_100_scaled = X_last_100_scaled.reshape(X_last_100.shape) # Reshape back to (100, 60, 1)

        X_test.shape, X_test_2D.shape, X_test_scaled.shape, X_last_100.shape, X_last_100_2D.shape,  X_last_100_scaled.shape

        # Build LSTM Model
        model = Sequential([
            LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),
            Dropout(0.2),
            LSTM(50, return_sequences=False),
            Dropout(0.2),
            Dense(25),
            Dense(1)
        ])

        model.compile(optimizer="adam", loss="mse")
        model.fit(X_train_scaled, y_train_scaled, epochs=20, batch_size=32, validation_data=(X_test_scaled, y_test_scaled))

        """#### Predict Stock Price of t+1 using LSTM"""

        # Predictions
        y_pred = model.predict(X_last_100_scaled)  # X_last_100_scaled now has the correct shape
        y_pred_inv = scaler.inverse_transform(y_pred)

        plt.plot(data.index[-100:], y_last_100, label="Actual")
        plt.plot(data.index[-100:], y_pred_inv, label="LSTM Prediction", linestyle="dashed")
        plt.legend()
        plt.show()

        """#### To Predict t+100 (Next 100 Days) using LSTM"""

        # Use the last available sequence from the test set as input
        last_seq = X_test_scaled[-1].astype(np.float32)  # Shape: (60, 1)
        future_predictions = []

        # Predict next 50 days
        for _ in range(100):
            next_pred = model.predict(last_seq.reshape(1, seq_length, 1))  # Predict 1 day ahead
            future_predictions.append(next_pred[0, 0])  # Save prediction

            # Update sequence: Remove first day & append new prediction
            last_seq = np.append(last_seq[1:], next_pred).reshape(seq_length, 1).astype(np.float32)

        # Inverse transform predictions to original scale
        future_predictions_inv = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))

        # Generate date range for future predictions
        # future_dates = pd.date_range(start=df.index[-1], periods=51)[1:]
        data_last_100 = data.iloc[-100:]
        future_dates = data_last_100.index

        # Plot Predictions
        plt.figure(figsize=(12, 6))
        plt.plot(future_dates, y_last_100, label="Past 100 Days Prices")
        plt.plot(future_dates, future_predictions_inv, label="Predicted Next 100 Days", linestyle="dashed", color='red')
        plt.title("Future Stock Price Predictions (Next 100 Days)")
        plt.legend()
        plt.show()

        # Print Predicted Prices
        predicted_prices_df = pd.DataFrame({"Date": future_dates, "Predicted_Close": future_predictions_inv.flatten(), "Actual_Close": y_last_100.flatten()})
        print(predicted_prices_df)

        # Plot Both Actual, LSTM, ARIMA predictions wrt Date
        plt.figure(figsize=(12, 6))
        plt.plot(data.index[-100:], data['Close'].values[-100:], label="Past 100 Days Prices")
        plt.plot(future_dates, future_predictions_inv, label="Predicted Next 100 Days-LSTM", linestyle="dashed", color='red')
        plt.plot(future_dates, forecast_values, label="Predicted Next 100 Days-ARIMA", linestyle="dashed", color='green')
        plt.title("Future Stock Price Predictions (Next 100 Days)")
        plt.legend()
        plt.show()

        # Final Forecast DF
        final_forecast_df = pd.DataFrame({"Date": future_dates, "Actual_Close": y_last_100.flatten(),
                                        "Predicted_Close_LSTM": future_predictions_inv.flatten(),
                                        "Predicted_Close_ARIMA": forecast_values})
        print(final_forecast_df)

        # Save final_forecast_df
        final_forecast_df.to_csv("~/ML_spark/dag_spark/Causal_Alpha/dataset/base_models/final_forecast_df.csv", index=False)


        # Evaluation Metrics
        def evaluate_model(y_true, y_pred):
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            mae = mean_absolute_error(y_true, y_pred)
            mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
            direction_accuracy = np.mean((np.sign(np.diff(y_true)) == np.sign(np.diff(y_pred))).astype(int)) * 100

            # Sharpe Ratio Calculation
            returns = np.diff(y_pred) / y_pred[:-1]
            sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) != 0 else 0

            r_squared = r2_score(y_true, y_pred)

            # Model Accuracy
            model_accuracy = 1 - (mae / np.mean(y_true))

            return {"RMSE": rmse, "MAE": mae, "MAPE": mape, "Directional Accuracy": direction_accuracy, "Sharpe Ratio": sharpe_ratio, "r_squared":r_squared, "Model Accuracy": model_accuracy}


            # return {"RMSE": rmse, "MAE": mae, "MAPE": mape, "Directional Accuracy": direction_accuracy, "Sharpe Ratio": sharpe_ratio}

        evaluate_model(final_forecast_df["Actual_Close"], final_forecast_df["Predicted_Close_LSTM"]) # LSTM Metrics

        evaluate_model(final_forecast_df["Actual_Close"], final_forecast_df["Predicted_Close_ARIMA"]) # ARIMA Metrics

    status = extract()
    forecast_value = train_ARIMA(status)
    train_LSTM(forecast_value)
base_model()

